name: 'Update parsers and test new back-release (on test server)'

on:
  workflow_dispatch:
  workflow_call:
  push:
    branches:
      - scrapy-standalone

env:
  TMP_DIR: '/tmp/download'
      
jobs:
  update_and_test_parsers:
    runs-on: 
      labels: test
    steps:

      - name: get current date-time and set it to env var
        run: echo "NOW=$(date +'%Y-%m-%dT%H-%M-%S')" >> $GITHUB_ENV

      - name: check temp dir exist and create it if not
        run: |
          if [ ! -d $TMP_DIR ]; then
            sudo mkdir $TMP_DIR
            echo "$TMP_DIR was not found. Created"
          fi

      - name: backup priveus version of scrapy
        run: |
          echo 'current date and time is ${{ env.NOW }}'
          sudo cp -R /usr/data/scrapy /usr/data/scrapy_app-${{ env.NOW }}
          echo 'previous version backup of scrapy is done'

      - name: Get archive of new version of scrapy and unzip it
        run: |
          sudo su && cd $TMP_DIR && sudo wget https://github.com/The-Conference/findconf/archive/refs/heads/scrapy-standalone.zip
          sudo unzip -o $TMP_DIR/scrapy-standalone.zip
          echo 'The new version of scrapy was got'

      - name: Stop containers and remove images of scrapy apps
        run: |
          sudo su && cd /usr/data && sudo docker-compose down
          sudo docker rmi $(sudo docker images -q data-scrapy:latest)
          echo "Container app_scrapy was stoped. Image was removed"

      - name: Remove old version of scrapy
        run: |
          sudo rm -rf /usr/data/scrapy/*
          echo "clean up old version of scrapy app"

      - name: Update scrapy with new version
        run: |
          sudo cp -R $TMP_DIR/findconf-scrapy-standalone/* /usr/data/scrapy
          sudo rm -rf /usr/data/scrapy/.gitignore /usr/data/scrapy/docker-compose.yml /usr/data/scrapy/README.md
          sudo cp -rf /root/scrapy/settings.py /usr/data/scrapy/conf_parsers/
          echo "all files of scrapy project was updated"

      - name: Cleanup temp files of scrapy project
        run: |
          sudo rm -rf $TMP_DIR/findconf-scrapy-standalone $TMP_DIR/scrapy-standalone.zip
          echo "Temp files was cleaned"

      - name: Up all containers
        run: |
          sudo su && cd /usr/data && sudo docker-compose up -d
          echo "Container started"
          sleep 60
          sudo docker ps
          sudo su && cd /usr/data && sudo docker-compose logs

      - name: Check django container uptime
        run: echo "EXIT_CODE=$(sudo docker inspect --format='{{.State.ExitCode}}' app_django)" >> $GITHUB_ENV

      - name: Check scrapy container uptime
        run: echo "EXIT_CODE_PARSER=$(sudo docker inspect --format='{{.State.ExitCode}}' app_parsers)" >> $GITHUB_ENV

      - name: check site http code
        run: echo "HTTP_CODE=$(curl -i -k --connect-timeout 20 -m 30  https://test.theconf.ru/admin/Conference_data/conference/ -o /dev/null -w '%{http_code}\n' -s)" >> $GITHUB_ENV

      - name: print success message if http status code is 302 and containers exit code is 0
        run: |
          echo 'app_django container exit code is ${{ env.EXIT_CODE }}'
          echo 'app_parsers container exit code is ${{ env.EXIT_CODE_PARSER }}'
          echo 'http status code is ${{ env.HTTP_CODE }}'
          echo 'Backend updated successful!'
          echo "PF_CODE=0" >> $GITHUB_ENV
        if: ${{ env.HTTP_CODE == 302 && env.EXIT_CODE == 0 && env.EXIT_CODE_PARSER == 0 }}

      - name: restore privious version if http status code isn't 302 or containers exit code isn't 0
        run: |
          echo 'http status code is ${{ env.HTTP_CODE }}'
          echo 'app_django container exit code is ${{ env.EXIT_CODE }}.'
          echo 'app_parsers container exit code is ${{ env.EXIT_CODE_PARSER }}'
          echo 'Scrapy updated failed! Restoring backup'
          sudo su && cd /usr/data && sudo docker-compose down
          sudo docker rmi $(sudo docker images -q data-scrapy:latest)
          echo "Container was stoped. Image was removed"
          sudo rm -rf /usr/data/scrapy/*
          sudo cp -R /usr/data/scrapy_app-${{ env.NOW }}/* /usr/data/scrapy
          echo 'Files of scrapy app was restored'
          sudo su && cd /usr/data && sudo docker-compose up -d
          sudo docker ps
          echo "Containers started"
          echo "PF_CODE=1" >> $GITHUB_ENV
        if: ${{ env.HTTP_CODE != 302 || env.EXIT_CODE != 0 || env.EXIT_CODE_PARSER != 0 }}

      - name: clean up backup
        run: |
          echo 'start cleaning up directory /usr/data/scrapy_app-${{ env.NOW }}'
          sudo rm -rf /usr/data/scrapy_app-${{ env.NOW }} 
          echo 'clean up completed'

      - name: Job successful finished
        run: echo 'Exit code is ${{ env.PF_CODE }}. Job finished success!'
        if: ${{ env.PF_CODE == 0 }}

      - name: Job finished failure
        run: |
          echo 'Exit code is ${{ env.PF_CODE }}. Job failed!'
          exit 1
        if: ${{ env.PF_CODE == 1 }}
